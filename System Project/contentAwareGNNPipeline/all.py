"""
  Contains Modular Classes for:

  DocumentTopicGenerator:

  KMeansClusterGenerator:

  DataPreProcessor: Preprocesses the data
  -- Builds the node dataframe which contains:
    -- node name
    -- each of it's attributes
  -- Builds the edge dataframe which contains:
    -- node source
    -- node target
    -- each edge type

  DataFrameAdaptor: Adapting Data For the Graph
  -- Defines nodes by index
  -- Updates graph edge node references to point to ids by index
  -- Generates training and test sets
  -- Generates a full adjacency matrix for testing

  TFGraph: Builds the graph structure and deep learning layers
  -- Builds the graph structure using tensors
  -- Builds the deep learning layers using Keras

  TFGNN: Exposes API methods to work with the model:
  -- _get_model
  -- _compile
  -- _fit (train)
  -- _predict

  @param training_data_df: pandas dataframe
  @param training_data_df: pandas dataframe
  @param is_node_prediction: boolean
  @return: node dataframe, edge dataframe
"""


class CategoryAwareTFGNN():
    def __init__(self, training_data_df, labels, is_node_prediction=True):
        data_df = training_data_df
        data_df['text_preprocessed'] = self._clean_text(data_df['text'])
        data_df['embedding'] = [self._tokenize(token) for token in data_df['text_preprocessed']]
        data_df = self._DocumentTopicGenerator(data_df=data_df)._update_df()
        data_df['kmeans_gen_cluster'] = self._KMeansClusterGenerator(self, data_df=data_df)._generate()
        self.data_df = data_df
        index_by_topic, topic_by_index = self._get_index_topic_maps()
        self.index_by_topic = index_by_topic
        self.topic_by_index = topic_by_index

        self.df_category_labels = labels

        # Preprocess data into nodes and edges
        dataPreProcessor = self._DataPreProcessor(outer_self=self)
        dataPreProcessor._process()

        # Adapt dataframes for graph
        self.node_df = dataPreProcessor.nodes
        self.edge_df = dataPreProcessor.edges
        self.dataFrameAdaptor = self._DataFrameAdaptor(outer_self=self)

        # Build TF-Graph
        self.tfGraph = self._TFGraph(outer_self=self)

        # Set up datasets.  Edge predictions can be requested by @param is_node_prediction
        training_dataset = self.tfGraph.train_node_dataset
        validation_dataset = self.tfGraph.full_node_dataset
        prediction_dataset = self.tfGraph.full_node_dataset
        if not is_node_prediction:
            training_dataset = self.tfGraph.train_edge_dataset
            validation_dataset = self.tfGraph.full_edge_dataset
            prediction_dataset = self.tfGraph.full_edge_dataset

        # Build the graph tensors and keras layers
        self.tfGNN = self._TFGNN(outer_self=self, num_graph_updates=3, training_dataset=training_dataset,
                                 validation_dataset=validation_dataset, prediction_dataset=prediction_dataset)

        """
          Compiles, trains, and generates predictions from the model.
          This can be called after instantiating the CategoryAwareTFGNN for the pipeline results.
          @return: predictions from the model generated by the pipeline
        """

    def _process(self):
        self.tfGNN._compile()
        self.tfGNN._fit()
        return self.tfGNN._predict()

    def _clean_text(self, column):
        df_column_without_punctuation = column.map(lambda x: re.sub('[,\.!?]', '', x))
        return df_column_without_punctuation.map(lambda x: x.lower())

    def _tokenize(self, token):
        word = Sentence(token)
        bert_embedding.embed(word)
        return word[0].embedding.numpy()

    def _get_index_topic_maps(self):
        dataset_length = len(self.data_df.index)
        topics = self.data_df['text_topic_1'].values.tolist() + self.data_df['text_topic_2'].values.tolist() + \
                 self.data_df['text_topic_3'].values.tolist()
        topics = sum(
            [[self.data_df['text_topic_1'][i], self.data_df['text_topic_2'][i], self.data_df['text_topic_3'][i]] for i
             in range(0, dataset_length)], [])
        index_by_topic = {}
        for indx, topic in enumerate(topics):
            if topic not in index_by_topic:
                index_by_topic[topic] = indx
        topic_by_index = {v: k for k, v in index_by_topic.items()}
        return index_by_topic, topic_by_index

    """
      Validates clusters against category labels using the edges to see
      if the categorization is shared between nodes.
  
      This validating assumes that a category is a dimensional space which can be
      have boundaries represented by the dimensions of text token embeddings.
  
      @return: The % accuracy of whether the category and cluster are both
      shared between nodes.
    """

    def _validate_clustering(self):
        cluster_consistent_with_catgory_count = len(
            self.edge_df[self.edge_df["shares_category"] == self.edge_df["shares_cluster"]])
        return cluster_consistent_with_catgory_count / len(self.edge_df)

    def _validate_topic_clusters(self):
        # map {cluster number: topic embeddings []}
        t_embed_by_cluster = {}
        df = self.node_df.filter(items=['cluster', "text_topic_1_index", "text_topic_2_index", "text_topic_3_index"])
        for i, row in enumerate(df.itertuples(), 1):
            cluster = row.cluster
            topic_embeddings = [
                self._tokenize(self.topic_by_index[row.text_topic_1_index]),
                self._tokenize(self.topic_by_index[row.text_topic_2_index]),
                self._tokenize(self.topic_by_index[row.text_topic_3_index])]
            if cluster in t_embed_by_cluster:
                t_embed_by_cluster[cluster] = t_embed_by_cluster[cluster] + topic_embeddings
            else:
                t_embed_by_cluster[cluster] = topic_embeddings

        # map {cluster number: topic embedding mean []}
        t_embed_mean_by_cluster = {}
        for cluster in t_embed_by_cluster:
            t_embed_mean_by_cluster[cluster] = np.mean(np.array(t_embed_by_cluster[cluster]))

        # map {cluster number: topic embedding deviations from mean []}
        t_embed_deviation_from_mean_by_cluster = {}
        for i, row in enumerate(df.itertuples(), 1):
            cluster = row.cluster
            topic_embeddings = [
                self._tokenize(self.topic_by_index[row.text_topic_1_index]),
                self._tokenize(self.topic_by_index[row.text_topic_2_index]),
                self._tokenize(self.topic_by_index[row.text_topic_3_index])]
            topic_embeddings_mean = t_embed_mean_by_cluster[cluster] - topic_embeddings
            t_embed_deviation_from_mean_by_cluster[cluster] = np.absolute(
                np.array(topic_embeddings) - np.array(topic_embeddings_mean))

        # average deviation from the mean of all embedding values for all topics in the cluster
        return {k: sum(sum(v)) / len(t_embed_deviation_from_mean_by_cluster) for k, v in
                t_embed_deviation_from_mean_by_cluster.items()}

    """
      Generates topics from each text sample using Linear Discriminant Analysis (LDA) 
      via a Gensim LDA Multicore Model
    """

    class _DocumentTopicGenerator():
        def __init__(self, data_df):
            self.stop_words = stopwords.words('english')
            self.stop_words.extend(['from', 'subject', 're', 'edu', 'use'])
            self.data_df = data_df

        def _update_df(self):
            self.data_df['text_topics'] = [self._predict_topics_from_gensim_lda_model(text) for text in
                                           self.data_df['text_preprocessed']]
            split = pd.DataFrame(self.data_df['text_topics'].to_list(),
                                 columns=['text_topic_1', 'text_topic_2', 'text_topic_3'])
            return self._clean_up_df(pd.merge(self.data_df, split, how='left', left_index=True, right_index=True))

        """
          Removes tokens which are too long or short
        """

        def _gensim_preprocess(self, sentences):
            for sentence in sentences:
                yield (gensim.utils.simple_preprocess(str(sentence), deacc=True))

        def _remove_stopwords(self, texts):
            return [[word for word in simple_preprocess(str(doc))
                     if word not in self.stop_words] for doc in texts]

        def _predict_topics_from_gensim_lda_model(self, text):
            data = [text]
            data_words = list(self._gensim_preprocess(data))
            data_words = self._remove_stopwords(data_words)
            id2word = corpora.Dictionary(data_words)
            texts = data_words
            corpus = [id2word.doc2bow(text) for text in texts]
            num_topics = 1
            lda_model = gensim.models.LdaMulticore(corpus=corpus,
                                                   id2word=id2word,
                                                   num_topics=num_topics)
            # Collect the topics from the first set provided in the list of topics.
            # A number of these are similar for this kind of corpus which is merely a
            # few sentences.
            topics = list(map(lambda tup: tup[1], lda_model.top_topics(corpus)[0][0]))
            # Just get the top 3 of the topics for now. It would be worth investigating
            # the effectiveness of adding additional topics.
            return topics[:3]

        def _clean_up_df(self, updated_pd):
            # Clean up the dataframe by imputing null values and removing columns that have been replaced
            topic_names = ['text_topic_1', 'text_topic_2', 'text_topic_3']
            for topic in topic_names:
                updated_pd[topic] = [n if n is not None else "" for n in updated_pd[topic]]
            used_columns = ['text_topics', 'text']
            for used_column in used_columns:
                updated_pd.drop([used_column], axis=1, inplace=True)
            return updated_pd

    """
      Uses KMeans to generate clusters from the text entities
    """

    class _KMeansClusterGenerator():
        def __init__(self, outer_self, data_df, num_clusters=4):
            self.num_clusters = num_clusters
            self.data_df = data_df

        def _generate(self):
            return self._find_clusters()

        def _find_clusters(self):
            text_token_segments = [text_preprocessed for text_preprocessed in self.data_df['embedding']]
            kclusterer = KMeansClusterer(self.num_clusters, distance=nltk.cluster.util.cosine_distance, repeats=25)
            return kclusterer.cluster(text_token_segments, assign_clusters=True)

    """
      Manages the preprocessing to collect and generate nodes and edges
    """

    class _DataPreProcessor():
        def __init__(self, outer_self):

            self.data_df = outer_self.data_df
            self.labels = outer_self.df_category_labels
            self.df_category_labels = outer_self.df_category_labels
            self.index_by_topic_map = outer_self.index_by_topic

        def _process(self):
            nodes, edges = self._process_training_data_to_graph_entities()
            self.nodes = nodes
            self.edges = edges

        """
          Builds the node and edge dataframes from the data.
          The node dataframe:
            -- node name
            -- each of it's attributes
              -- topics
          The edge dataframe:
            -- node source
            -- node target
            -- each edge type
          @return: node dataframe, edge dataframe
        """

        def _process_training_data_to_graph_entities(self):
            # Set up matrix variables
            node_cols = ["text_name", "category_num", 'cluster', "text_topic_1_index", "text_topic_2_index",
                         "text_topic_3_index"]
            # category_node_cols = ["category_name", "category_num"]
            graph_df_nodes = pd.DataFrame(columns=node_cols)
            edge_cols = ["source", "target", "shares_category", "shares_cluster", "cos_sim_target_with_source"]
            graph_df_edges = pd.DataFrame(columns=edge_cols)
            text_index = 0

            for text_index_i in range(0, len(self.data_df)):

                text_embedding_i = self.data_df['embedding'][text_index_i]

                category_name_i = self.labels[self.data_df['label'][text_index_i]]
                text_category_i = self.df_category_labels[self.data_df['label'][text_index_i]]
                label_i = self.data_df['label'][text_index_i]
                cluster_i = self.data_df['kmeans_gen_cluster'][text_index_i]

                graph_df_nodes.loc[len(graph_df_nodes.index)] = [
                    # text name
                    "text-" + str(text_index_i),
                    # category num
                    self.data_df['label'][text_index_i].astype(np.int32),
                    # cluster
                    cluster_i.astype(np.int32),
                    # text topic 1 index
                    np.int32(self.index_by_topic_map[self.data_df['text_topic_1'][text_index_i]]),
                    # text topic 2 index
                    np.int32(self.index_by_topic_map[self.data_df['text_topic_2'][text_index_i]]),
                    # text topic 3 index
                    np.int32(self.index_by_topic_map[self.data_df['text_topic_3'][text_index_i]])
                ]
                for text_index_j in range(0, len(self.data_df)):
                    text_embedding_j = self.data_df['embedding'][text_index_j]

                    text_category_j = self.df_category_labels[self.data_df['label'][text_index_j]]
                    label_j = self.data_df['label'][text_index_j]
                    cluster_j = self.data_df['kmeans_gen_cluster'][text_index_j]

                    shares_category_binary = 0
                    if label_i == label_j:
                        shares_category_binary = 1

                    shares_cluster_binary = 0
                    if cluster_i == cluster_j:
                        shares_cluster_binary = 1

                    graph_df_edges.loc[len(graph_df_edges.index)] = [
                        # source
                        "text-" + str(text_index_i),
                        # target
                        "text-" + str(text_index_j),
                        # shares category
                        np.int32(shares_category_binary),
                        # shares_cluster
                        np.int32(shares_cluster_binary),
                        # cos_sim_target_with_source,
                        self._cos_similarity(text_embedding_i, text_embedding_j)
                    ]
            return graph_df_nodes, graph_df_edges

        def _squared_sum(self, x):
            return round(sqrt(sum([a * a for a in x])), 3)

        def _cos_similarity(self, x, y):
            return round(sum(a * b for a, b in zip(x, y)) / float(self._squared_sum(x) * self._squared_sum(y)), 3)

    """
      Builds the graph structure for the GNN from the node and edge dataframes using tfgnn.GraphTensor, tfgnn.NodeSet, and tfgnn.EdgeSet.
      Builds the deep learning layers for the GNN using Keras and makes these available for fitting the TF-GNN model
    """

    class _TFGraph():
        def __init__(self, outer_self):
            full_tensor = self._create_graph_tensor(outer_self.dataFrameAdaptor.node_full_adj,
                                                    outer_self.dataFrameAdaptor.edge_full_adj)
            train_tensor = self._create_graph_tensor(outer_self.dataFrameAdaptor.node_train_adj,
                                                     outer_self.dataFrameAdaptor.edge_train_adj)

            self.full_node_dataset = self._generate_dataset_from_graph(full_tensor, self._node_batch_merge)
            self.train_node_dataset = self._generate_dataset_from_graph(train_tensor, self._node_batch_merge)

            self.full_edge_dataset = self._generate_dataset_from_graph(full_tensor, self._edge_batch_merge)
            self.train_edge_dataset = self._generate_dataset_from_graph(train_tensor, self._edge_batch_merge)

            self.set_initial_node_state = self._set_initial_node_state
            self.set_initial_edge_state = self._set_initial_edge_state

            graph_spec = self.train_edge_dataset.element_spec[0]
            self.input_graph = tf.keras.layers.Input(type_spec=graph_spec)
            print("graph spec compatibility: ", graph_spec.is_compatible_with(full_tensor))

            self.dense_layer = self._dense_layer

        def _create_graph_tensor(self, node_df, edge_df):
            graph_tensor = tfgnn.GraphTensor.from_pieces(
                node_sets={
                    "articles": tfgnn.NodeSet.from_fields(
                        sizes=[len(node_df)],
                        features={
                            'article_category': np.array(node_df['category_num'], dtype='int32').reshape(len(node_df),
                                                                                                         1),
                            'article_cluster': np.array(node_df['cluster'], dtype='int32').reshape(len(node_df), 1),
                            'article_topic_1_id': np.array(node_df['text_topic_1_index'], dtype='int32').reshape(
                                len(node_df), 1),
                            'article_topic_2_id': np.array(node_df['text_topic_2_index'], dtype='int32').reshape(
                                len(node_df), 1),
                            'article_topic_3_id': np.array(node_df['text_topic_3_index'], dtype='int32').reshape(
                                len(node_df), 1)
                        })
                },
                edge_sets={
                    "topics": tfgnn.EdgeSet.from_fields(
                        sizes=[len(edge_df)],
                        features={
                            'topics_shared': np.array(edge_df['shares_category'],
                                                      dtype='int32').reshape(len(edge_df), 1),
                            'cosine_similarity': np.array(edge_df['cos_sim_target_with_source'],
                                                          dtype='float32').reshape(len(edge_df), 1)},
                        adjacency=tfgnn.Adjacency.from_indices(
                            source=("articles", np.array(edge_df['source_id'], dtype='int32')),
                            target=("articles", np.array(edge_df['target_id'], dtype='int32'))))
                }
            )
            return graph_tensor

        def _node_batch_merge(self, graph):
            graph = graph.merge_batch_to_components()
            node_features = graph.node_sets['articles'].get_features_dict()
            edge_features = graph.edge_sets['topics'].get_features_dict()

            label = node_features.pop('article_category')
            _ = edge_features.pop('topics_shared')

            new_graph = graph.replace_features(
                node_sets={'articles': node_features},
                edge_sets={'topics': edge_features})
            return new_graph, label

        def _edge_batch_merge(self, graph):
            graph = graph.merge_batch_to_components()
            node_features = graph.node_sets['articles'].get_features_dict()
            edge_features = graph.edge_sets['topics'].get_features_dict()

            _ = node_features.pop('article_category')
            label = edge_features.pop('topics_shared')

            new_graph = graph.replace_features(
                node_sets={'articles': node_features},
                edge_sets={'topics': edge_features})
            return new_graph, label

        def _generate_dataset_from_graph(self, graph, function):
            dataset = tf.data.Dataset.from_tensors(graph)
            dataset = dataset.batch(32)
            return dataset.map(function)

        def _set_initial_node_state(self, node_set, node_set_name):
            features = [
                tf.keras.layers.Dense(32, activation="relu")(node_set['article_cluster']),
                tf.keras.layers.Dense(32, activation="relu")(node_set['article_topic_1_id']),
                tf.keras.layers.Dense(32, activation="relu")(node_set['article_topic_2_id']),
                tf.keras.layers.Dense(32, activation="relu")(node_set['article_topic_3_id'])
            ]
            return tf.keras.layers.Concatenate()(features)

        def _set_initial_edge_state(self, edge_set, edge_set_name):
            features = [
                tf.keras.layers.Dense(32, activation="relu")(edge_set['cosine_similarity'])
            ]
            return tf.keras.layers.Concatenate()(features)

        def _dense_layer(self, units=64, l2_reg=0.1, dropout=0.25, activation='relu'):
            regularizer = tf.keras.regularizers.l2(l2_reg)
            return tf.keras.Sequential([
                tf.keras.layers.Dense(units,
                                      kernel_regularizer=regularizer,
                                      bias_regularizer=regularizer),
                tf.keras.layers.Dropout(dropout)])

    """
      Exposes API methods to:
      -- Build the model
      -- Compile the model
      -- Fit the model (training)
      -- Predicting from the model
  
      These methods need to be called in order.  However multiple predictions can be made on the same model.
    """

    class _TFGNN():
        def __init__(self, outer_self, num_graph_updates, training_dataset, validation_dataset, prediction_dataset,
                     steps=10, epochs=100, loss=None, metrics=None):
            # Graph update variables
            self.input_graph = outer_self.tfGraph.input_graph
            self.dense_layer = outer_self.tfGraph.dense_layer
            self.set_initial_node_state = outer_self.tfGraph.set_initial_node_state
            self.set_initial_edge_state = outer_self.tfGraph.set_initial_edge_state

            self.num_graph_updates = num_graph_updates
            self.model = self._get_model()

            self.training_dataset = training_dataset
            self.validation_dataset = validation_dataset
            self.prediction_dataset = prediction_dataset

            # Training params
            self.steps_per_epoch = steps
            self.epochs = epochs

        def _get_model(self):
            graph = tfgnn.keras.layers.MapFeatures(
                node_sets_fn=self.set_initial_node_state,
                edge_sets_fn=self.set_initial_edge_state
            )(self.input_graph)

            graph_updates = self.num_graph_updates
            for i in range(graph_updates):
                graph = tfgnn.keras.layers.GraphUpdate(
                    node_sets={
                        'articles': tfgnn.keras.layers.NodeSetUpdate({
                            'topics': tfgnn.keras.layers.SimpleConv(
                                message_fn=self.dense_layer(32),
                                reduce_type="sum",
                                sender_edge_feature=tfgnn.HIDDEN_STATE,
                                receiver_tag=tfgnn.TARGET)},
                            tfgnn.keras.layers.NextStateFromConcat(
                                self.dense_layer(64)))})(graph)  # start here

                logits = tf.keras.layers.Dense(1, activation='softmax')(graph.node_sets["articles"][tfgnn.HIDDEN_STATE])
            return tf.keras.Model(self.input_graph, logits)

        def _compile(self):
            self.model.compile(
                tf.keras.optimizers.Adam(learning_rate=0.3),
                # loss = 'categorical_crossentropy',
                # metrics = ['categorical_accuracy']
            )

        def _fit(self):
            es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=100,
                                                  restore_best_weights=True)
            self.model.fit(self.training_dataset.repeat(),
                           validation_data=self.validation_dataset,
                           steps_per_epoch=self.steps_per_epoch,
                           epochs=self.epochs,
                           callbacks=[es])

        def _predict(self):
            return self.model.predict(self.prediction_dataset)